{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Project - CAS 764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from nltk.classify import MaxentClassifier\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import *\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.metrics.scores import (precision, recall)\n",
    "import collections\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "import random\n",
    "from random import randint\n",
    "import re\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datapath1 = './NEEL2006/'\n",
    "datapath2 = './CoNLL2003/'\n",
    "\n",
    "neel_training_file = open(datapath1 + 'NEEL2016-training.tsv', 'r')\n",
    "neel_training_data = neel_training_file.readlines()\n",
    "neel_training_file.close()\n",
    "\n",
    "neel_training_labels_file = open(datapath1 + 'NEEL2016-training_neel.gs', 'r')\n",
    "neel_training_labels_data = neel_training_labels_file.readlines()\n",
    "neel_training_labels_file.close()\n",
    "\n",
    "neel_validation_file = open(datapath1 + 'NEEL2016-dev.tsv', 'r')\n",
    "neel_validation_data = neel_validation_file.readlines()\n",
    "neel_validation_file.close()\n",
    "\n",
    "neel_validation_labels_file = open(datapath1 + 'NEEL2016-dev_neel.gs', 'r')\n",
    "neel_validation_labels_data = neel_validation_labels_file.readlines()\n",
    "neel_validation_labels_file.close()\n",
    "\n",
    "neel_testing_file = open(datapath1 + 'NEEL2016-test.tsv', 'r')\n",
    "neel_testing_data = neel_testing_file.readlines()\n",
    "neel_testing_file.close()\n",
    "\n",
    "neel_testing_labels_file = open(datapath1 + 'NEEL2016-test_neel.gs', 'r')\n",
    "neel_testing_labels_data = neel_testing_labels_file.readlines()\n",
    "neel_testing_labels_file.close()\n",
    "\n",
    "conll_training_file = open(datapath2 + 'eng.train', 'r')\n",
    "conll_training_data = conll_training_file.readlines()\n",
    "conll_training_file.close()\n",
    "\n",
    "conll_testA_file = open(datapath2 + 'eng.testa', 'r')\n",
    "conll_testA_data = conll_testA_file.readlines()\n",
    "conll_testA_file.close()\n",
    "\n",
    "conll_testB_file = open(datapath2 + 'eng.testb', 'r')\n",
    "conll_testB_data = conll_testB_file.readlines()\n",
    "conll_testB_file.close()\n",
    "\n",
    "conll_testC_file = open(datapath2 + 'eng.testc', 'r')\n",
    "conll_testC_data = conll_testC_file.readlines()\n",
    "conll_testC_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conll_preprocess(data):\n",
    "    data_dummy = data[:]\n",
    "    for i in range(len(data)):\n",
    "        if data[i] != \"\\n\":\n",
    "            split_data = data[i].split()\n",
    "            ne_tag = split_data[-1]\n",
    "            if ne_tag == 'I-ORG':\n",
    "                split_data[-1] = 'ORG'\n",
    "            elif ne_tag == 'I-MISC':\n",
    "                split_data[-1] = 'MISC'\n",
    "            elif ne_tag == 'I-PER':\n",
    "                split_data[-1] = 'PER'\n",
    "            elif ne_tag == 'I-LOC':\n",
    "                split_data[-1] = 'LOC'\n",
    "            elif ne_tag == 'B-ORG':\n",
    "                split_data[-1] = 'ORG'\n",
    "            elif ne_tag == 'B-MISC':\n",
    "                split_data[-1] = 'MISC'\n",
    "            elif ne_tag == 'B-LOC':\n",
    "                split_data[-1] = 'LOC'\n",
    "            else:\n",
    "                split_data[-1] = 'O'\n",
    "            data_dummy[i] = \" \".join(split_data)\n",
    "    return data_dummy\n",
    "\n",
    "conll_training = conll_preprocess(conll_training_data) \n",
    "conll_testing = conll_preprocess(conll_testA_data) + conll_preprocess(conll_testB_data) + conll_preprocess(conll_testC_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/marshallwice/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "def neel_preprocess(data):\n",
    "    processed = []      \n",
    "    for i in range(len(data)):\n",
    "        if data[i] != '\\n':\n",
    "            line = data[i].rstrip()\n",
    "            split_index = line.find(',')\n",
    "            entry = [line[1:split_index-1], line[split_index+2:-1]]\n",
    "            processed.append(entry)\n",
    "    return processed\n",
    "\n",
    "def neel_preprocess_labels(data):\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] != '\\n':\n",
    "            line = data[i].rstrip().split()\n",
    "            t = line[5]\n",
    "            if t == 'Person':\n",
    "                t = 'PER'\n",
    "            elif t == 'Product':\n",
    "                t = 'MISC'\n",
    "            elif t == 'Thing':\n",
    "                t = 'MISC'\n",
    "            elif t == 'Organization':\n",
    "                t = 'ORG'\n",
    "            elif t == 'Location':\n",
    "                t = 'LOC'\n",
    "            elif t == 'Character':\n",
    "                t = 'PER'\n",
    "            elif t == 'Event':\n",
    "                t = 'MISC'\n",
    "            else:\n",
    "                t = 'O'\n",
    "            labels.append([line[0], line[1], line[2], t])\n",
    "    return labels\n",
    "\n",
    "def generate_word_labels(data, labels):\n",
    "    all_labels = []\n",
    "    for i in range(len(data)): \n",
    "        sentence_labels = []\n",
    "        for word in range(len(data[i][1].split())):\n",
    "            sentence_labels.append('O')\n",
    "        for j in range(len(labels)): \n",
    "            if data[i][0] == labels[j][0]:\n",
    "                start_index = int(labels[j][1])\n",
    "                end_index = int(labels[j][2])\n",
    "                w = data[i][1][start_index:end_index].split()\n",
    "                for ele in range(len(w)):\n",
    "                    ind = [idx for idx, wo in enumerate(data[i][1].split(), 1) if w[ele] in wo]\n",
    "                    for index in ind:\n",
    "                        sentence_labels[index-1] = labels[j][3]\n",
    "        all_labels.append(sentence_labels)\n",
    "    return all_labels\n",
    "\n",
    "def generate_pos_tags(data):\n",
    "    all_pos_tags = []\n",
    "    for i in range(len(data)):\n",
    "        sentence_tags = nltk.pos_tag(data[i][1].split())\n",
    "        tag_list = []\n",
    "        for tag in range(len(sentence_tags)):\n",
    "            tag_list.append(sentence_tags[tag][1])\n",
    "        all_pos_tags.append(tag_list)\n",
    "    return all_pos_tags\n",
    "\n",
    "def reformat(data, labels, pos_tags):\n",
    "    reformatted_dataset = []\n",
    "    for i in range(len(data)):\n",
    "        word_list = data[i][1].split()\n",
    "        for word in range(len(word_list)):\n",
    "            output = word_list[word] + ' ' + pos_tags[i][word] + ' ' + ' X ' + labels[i][word]\n",
    "            reformatted_dataset.append(output)\n",
    "        reformatted_dataset.append('\\n')\n",
    "    return reformatted_dataset\n",
    "\n",
    "def convert(initial_dataset, initial_labelset):\n",
    "    a = neel_preprocess(initial_dataset)\n",
    "    b = neel_preprocess_labels(initial_labelset)\n",
    "    c = generate_word_labels(a, b)\n",
    "    d = generate_pos_tags(a)\n",
    "    e = reformat(a, c, d)\n",
    "    return e\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "neel_training = convert(neel_training_data, neel_training_labels_data)\n",
    "neel_testing = convert(neel_testing_data, neel_testing_labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    cleaned_data = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] != '\\n':\n",
    "            entry = data[i].split()\n",
    "            cleaned_word = ''.join(c for c in entry[0] if  c not in '!$%^&*-–=_’+“{}[]\\|;:,<.>/?`~\")(')\n",
    "            if (len(cleaned_word) != 0) and (cleaned_word[0] != '\\'') and (cleaned_word[-1] != '\\''):\n",
    "                clean_word = ''.join(c for c in cleaned_word if  c not in '\\'')\n",
    "                if ('http' not in clean_word) and (any(c.isalpha() for c in clean_word) == True) and (clean_word != 'RT'):\n",
    "                    if clean_word[0] == '#':\n",
    "                        cleaned_data.append(' '.join(['hashtag', entry[1], entry[2], entry[3]]))\n",
    "                    elif clean_word[0] == '@':\n",
    "                        cleaned_data.append(' '.join(['sign', entry[1], entry[2], entry[3]]))\n",
    "                    else:\n",
    "                        cleaned_data.append(' '.join([clean_word, entry[1], entry[2], entry[3]]))\n",
    "        else:\n",
    "            cleaned_data.append('\\n')\n",
    "    return cleaned_data\n",
    "\n",
    "cleaned_conll_training = clean(conll_training)\n",
    "cleaned_conll_testing = clean(conll_testing)\n",
    "\n",
    "cleaned_neel_training = clean(neel_training)\n",
    "cleaned_neel_testing = clean(neel_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = './glove/glove.6B.100d.txt'\n",
    "word2vec_output_file = './glove/glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "filename = './glove/glove.6B.100d.txt.word2vec'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def restructure(data):\n",
    "    matrix = []\n",
    "    cnn_X = []\n",
    "    cnn_Y = []\n",
    "    cnn_Z = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] != data[0]:\n",
    "            if data[i] != \"\\n\":    \n",
    "                matrix.append(data[i].rstrip().split())\n",
    "            elif len(matrix)>0:\n",
    "                matrix = np.array(matrix)\n",
    "                cnn_X.append(matrix[:,0])\n",
    "                cnn_Y.append(matrix[:,3])\n",
    "                cnn_Z.append(matrix[:,1])\n",
    "                matrix = []\n",
    "                \n",
    "    return cnn_X, cnn_Y, cnn_Z\n",
    "\n",
    "def vectorize(word_matrix):\n",
    "    matrixvex = []\n",
    "    for matrix in word_matrix:\n",
    "        wordvex = []\n",
    "        for w in range(matrix.shape[0]):\n",
    "            try:\n",
    "                wordvex.append(w2v_model[matrix[w].lower()])\n",
    "            except KeyError:\n",
    "                wordvex.append(w2v_model['the'])\n",
    "        matrixvex.append(np.array(wordvex))\n",
    "    return np.array(matrixvex)\n",
    "    \n",
    "def pad_words(dataset):\n",
    "    data_matrix = [] \n",
    "    for i in range(len(dataset)):\n",
    "        sentence = dataset[i]\n",
    "        sentence_matrix = []\n",
    "        if len(sentence) == 1:\n",
    "            word_matrix = np.zeros((3, 100))\n",
    "            word_matrix[1] = sentence\n",
    "            sentence_matrix.append(word_matrix)\n",
    "\n",
    "        elif len(sentence) == 2:\n",
    "            for i in range(len(sentence)):\n",
    "                word_matrix = np.zeros((3, 100))\n",
    "                if i == 0:\n",
    "                    word_matrix[1] = sentence[i]\n",
    "                    word_matrix[2] = sentence[i+1]\n",
    "                elif i == len(sentence)-1:\n",
    "                    word_matrix[0] = sentence[i-1]\n",
    "                    word_matrix[1] = sentence[i]\n",
    "                sentence_matrix.append(word_matrix)\n",
    "\n",
    "        else:\n",
    "            for i in range(len(sentence)):\n",
    "                word_matrix = np.zeros((3, 100))\n",
    "                if i == 0:\n",
    "                    word_matrix[1] = sentence[i]\n",
    "                    word_matrix[2] = sentence[i+1]\n",
    "                elif i == len(sentence)-1:\n",
    "                    word_matrix[0] = sentence[i-1]\n",
    "                    word_matrix[1] = sentence[i]\n",
    "                else:\n",
    "                    word_matrix[0] = sentence[i-1]\n",
    "                    word_matrix[1] = sentence[i]\n",
    "                    word_matrix[2] = sentence[i+1]\n",
    "                sentence_matrix.append(word_matrix)\n",
    "            \n",
    "        data_matrix.append(np.array(sentence_matrix)) \n",
    "        \n",
    "    data_matrix = np.array(data_matrix)\n",
    "    flattened_data_matrix = []\n",
    "    for i in range(len(data_matrix)):\n",
    "        for j in range(data_matrix[i].shape[0]):\n",
    "            flattened_data_matrix.append(data_matrix[i][j])\n",
    "            \n",
    "    return np.array(flattened_data_matrix)\n",
    "\n",
    "def pad_labels(dataset):\n",
    "    padded_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        for j in range(len(dataset[i])):\n",
    "            padded_dataset.append(dataset[i][j])\n",
    "    return np.array(padded_dataset)\n",
    "\n",
    "def cnn_preprocess(dataset):\n",
    "    cnn_x, cnn_y, cnn_z = restructure(dataset) \n",
    "    cnn_x = pad_words(vectorize(cnn_x))\n",
    "    cnn_x = cnn_x.reshape(cnn_x.shape[0], 3, 100, 1)\n",
    "    cnn_y = pad_labels(cnn_y)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(cnn_y)\n",
    "    encoded_Y = encoder.transform(cnn_y)\n",
    "    cnn_y = np_utils.to_categorical(encoded_Y)\n",
    "    return cnn_x, cnn_y\n",
    "\n",
    "cnn_train_x, cnn_train_y = cnn_preprocess(cleaned_conll_training + cleaned_neel_training)\n",
    "cnn_test_x, cnn_test_y = cnn_preprocess(cleaned_conll_testing + cleaned_neel_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(3, 100, 1)))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def train_cnn(train_x, train_y, batch_size, num_cycles):\n",
    "    cnn_model = create_model()\n",
    "    cnn_model.summary()\n",
    "    cnn_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    cnn_model.fit(train_x, train_y, batch_size=batch_size, epochs=num_cycles, shuffle=False, verbose=2)\n",
    "    return cnn_model\n",
    "\n",
    "def save_model(cnn_model, with_name):\n",
    "    model_json = cnn_model.to_json()\n",
    "    with open(with_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    cnn_model.save_weights(with_name + \".h5\")\n",
    "    \n",
    "def load_model(with_name):\n",
    "    json_file = open(with_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(with_name + \".h5\")\n",
    "    return loaded_model\n",
    "\n",
    "# Train Model & Save to File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# cnn_model = train_cnn(cnn_train_x, cnn_train_y, 32, 20)\n",
    "# save_model(cnn_model, 'combinedgeneralcnnmodel')\n",
    "\n",
    "# Load Model from File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "cnn_model = load_model('combinedgeneralcnnmodel')\n",
    "cnn_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 104500 out of 114442 giving an accuracy of 91.31%\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC       0.65      0.91      0.76      4062\n",
      "       MISC       0.59      0.51      0.55      2680\n",
      "          O       0.96      0.96      0.96     96665\n",
      "        ORG       0.86      0.42      0.56      4689\n",
      "        PER       0.65      0.80      0.71      6346\n",
      "\n",
      "avg / total       0.92      0.91      0.91    114442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def form_cnn(cnn_list):\n",
    "    new_list = []\n",
    "    for i in range(len(cnn_list)):\n",
    "        if cnn_list[i] == 0:\n",
    "            new_list.append('LOC')\n",
    "        elif cnn_list[i] == 1:\n",
    "            new_list.append('MISC')\n",
    "        elif cnn_list[i] == 2:\n",
    "            new_list.append('O')\n",
    "        elif cnn_list[i] == 3:\n",
    "            new_list.append('ORG')\n",
    "        elif cnn_list[i] == 4:\n",
    "            new_list.append('PER')\n",
    "    return new_list\n",
    "\n",
    "def cnn_evaluation(model, test_x, test_y):\n",
    "    num_correct = 0\n",
    "    for i in range(len(test_x)):\n",
    "        if np.where(test_y[i]==1)[0] == model.predict_classes(test_x[i].reshape(1, 3, 100, 1)):\n",
    "            num_correct+=1\n",
    "\n",
    "    acc = model.evaluate(test_x, test_y, verbose=2)[1]*100\n",
    "    \n",
    "    Y_test = np.argmax(test_y, axis=1)\n",
    "    Y_pred = model.predict_classes(test_x)\n",
    "    Y_test = form_cnn(Y_test)\n",
    "    Y_pred = form_cnn(Y_pred)\n",
    "    \n",
    "    print('Predicted', num_correct, 'out of', len(test_x), 'giving an accuracy of {}%'.format(round(acc, 2)))\n",
    "    print('\\n')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    \n",
    "cnn_evaluation(cnn_model, cnn_test_x, cnn_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy Markov Model (MEMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def memm_preprocess(data):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] != \"\\n\":    \n",
    "            new_data.append(data[i].rstrip().split())\n",
    "        else:\n",
    "            new_data.append(['-END-END-END-', '-END-', '-END-', '-END-'])\n",
    "    \n",
    "    new_data[:] = [x for x in new_data if x != ['-DOCSTART-', '-X-', 'O', 'O']]\n",
    "        \n",
    "    if new_data[0] == ['-END-END-END-', '-END-', '-END-', '-END-']:\n",
    "        new_data = np.delete(new_data, 0, 0)\n",
    "    \n",
    "    new_data = np.array(new_data)\n",
    "    if len(new_data[0]) == 4:\n",
    "        new_data = np.delete(new_data, 2, 1)\n",
    "\n",
    "    tuple_data = []\n",
    "    for i in range(len(new_data)):\n",
    "        tuple_data.append((new_data[i]))\n",
    "    \n",
    "    return tuple_data\n",
    "    \n",
    "memm_train = memm_preprocess(cleaned_conll_training+cleaned_neel_training)\n",
    "memm_test = memm_preprocess(cleaned_conll_testing+cleaned_neel_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def memm_find_features(current_word, next_word, previous_word, previous_netag):\n",
    "    return {'current_word':current_word, \n",
    "            'next_word':next_word, \n",
    "            'previous_word':previous_word, \n",
    "            'previous_netag':previous_netag}\n",
    "\n",
    "def fetch_featureset(dataset):\n",
    "    featureset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][0] != '-END-END-END-':\n",
    "            if i == 0:\n",
    "                current_word = dataset[i][0]\n",
    "                next_word = dataset[i+1][0]\n",
    "                previous_word = \"\"\n",
    "                previous_netag = \"\"\n",
    "            elif dataset[i-1][0] == '-END-END-END-':\n",
    "                current_word = dataset[i][0]\n",
    "                next_word = dataset[i+1][0] \n",
    "                previous_word = \"\"\n",
    "                previous_netag = \"\"\n",
    "            else:\n",
    "                try:\n",
    "                    current_word = dataset[i][0]\n",
    "                    if dataset[i+1][0] != '-END-END-END-':\n",
    "                        next_word = dataset[i+1][0] \n",
    "                    else:\n",
    "                        next_word = \"\"\n",
    "                    previous_word = dataset[i-1][0] \n",
    "                    previous_netag = dataset[i-1][2]\n",
    "                except IndexError:\n",
    "                    current_word = dataset[i][0]\n",
    "                    next_word = \"\"\n",
    "                    previous_word = dataset[i-1][0] \n",
    "                    previous_netag = dataset[i-1][2]\n",
    "            featureset.append([memm_find_features(current_word, next_word, previous_word, previous_netag), dataset[i][2]])\n",
    "    return featureset\n",
    "\n",
    "def fetch_test_featureset(dataset):\n",
    "    featureset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][0] != '-END-END-END-':\n",
    "            if i == 0:\n",
    "                current_word = dataset[i][0]\n",
    "                next_word = dataset[i+1][0]\n",
    "                previous_word = \"\"\n",
    "                previous_netag = \"\"\n",
    "            elif dataset[i-1][0] == '-END-END-END-':\n",
    "                current_word = dataset[i][0]\n",
    "                next_word = dataset[i+1][0] \n",
    "                previous_word = \"\"\n",
    "                previous_netag = \"\"\n",
    "            else:\n",
    "                try:\n",
    "                    current_word = dataset[i][0]\n",
    "                    if dataset[i+1][0] != '-END-END-END-':\n",
    "                        next_word = dataset[i+1][0] \n",
    "                    else:\n",
    "                        next_word = \"\"\n",
    "                    previous_word = dataset[i-1][0] \n",
    "                    previous_netag = memm_classifier.classify(memm_find_features(current_word, next_word, previous_word, previous_netag))\n",
    "                except IndexError:\n",
    "                    current_word = dataset[i][0]\n",
    "                    next_word = \"\"\n",
    "                    previous_word = dataset[i-1][0] \n",
    "                    previous_netag = memm_classifier.classify(memm_find_features(current_word, next_word, previous_word, previous_netag))\n",
    "            featureset.append([memm_find_features(current_word, next_word, previous_word, previous_netag), dataset[i][2]])\n",
    "    return featureset\n",
    "\n",
    "memm_test_featuresets = fetch_featureset(memm_test)\n",
    "        \n",
    "# Train Model & Save to File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# memm_train_featuresets = fetch_featureset(memm_train)\n",
    "# memm_classifier = nltk.classify.MaxentClassifier.train(memm_train_featuresets, max_iter=20)\n",
    "\n",
    "# memm_save_classifier = open('combinedmemmclassifier.pickle', 'wb')\n",
    "# pickle.dump(memm_classifier, memm_save_classifier)\n",
    "# memm_save_classifier.close()\n",
    "\n",
    "# Load Model from File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "memm_save_classifier_f = open('combinedmemmclassifier.pickle', 'rb')\n",
    "memm_classifier = pickle.load(memm_save_classifier_f)\n",
    "memm_save_classifier_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 104424 out of 114658 giving an accuracy of 91.07%\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC       0.84      0.73      0.78      4062\n",
      "       MISC       0.49      0.62      0.55      2680\n",
      "          O       0.95      0.96      0.95     96881\n",
      "        ORG       0.80      0.65      0.72      4689\n",
      "        PER       0.67      0.66      0.66      6346\n",
      "\n",
      "avg / total       0.91      0.91      0.91    114658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def memm_evaluate(testset, classifier, test_feats):\n",
    "    new_memm_test = []\n",
    "    for i in range(len(testset)):\n",
    "        if testset[i][0] != '-END-END-END-':\n",
    "            new_memm_test.append(testset[i])\n",
    "\n",
    "    num_correct = 0\n",
    "    pred_list = []\n",
    "    ground_truths = []\n",
    "    for i in range(len(new_memm_test)):\n",
    "        prediction = classifier.classify(test_feats[i][0])\n",
    "        pred_list.append(prediction)\n",
    "        ground_truths.append(new_memm_test[i][2])\n",
    "        if prediction == new_memm_test[i][2]:\n",
    "            num_correct = num_correct + 1\n",
    "    acc = (nltk.classify.accuracy(classifier, test_feats))*100\n",
    "    \n",
    "    print('Predicted', num_correct, 'out of', len(new_memm_test), 'giving an accuracy of {}%'.format(round(acc, 2)))\n",
    "    print('\\n')\n",
    "    print(classification_report(ground_truths, pred_list))\n",
    "\n",
    "memm_evaluate(memm_test, memm_classifier, memm_test_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification (NBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbc_preprocess(data):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] != \"\\n\":    \n",
    "            new_data.append(data[i].rstrip().split())\n",
    "    \n",
    "    new_data[:] = [x for x in new_data if x != ['-DOCSTART-', '-X-', 'O', 'O']]\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    if len(new_data[0]) == 4:\n",
    "        new_data = np.delete(new_data, 2, 1)\n",
    "\n",
    "    tuple_data = []\n",
    "    for i in range(len(new_data)):\n",
    "        tuple_data.append((new_data[i]))\n",
    "            \n",
    "    return tuple_data\n",
    "\n",
    "nbc_train = nbc_preprocess(cleaned_conll_training + cleaned_neel_training)\n",
    "nbc_test = nbc_preprocess(cleaned_conll_testing + cleaned_neel_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbc_find_features(word, pos):\n",
    "    return {'first_letter': word[0], 'pos': pos, 'word':word}\n",
    "\n",
    "def nbc_train_classifier(trainset, testset):\n",
    "    train_feats = [(nbc_find_features(word, pos_tag), ne_tag) for (word, pos_tag, ne_tag) in trainset]\n",
    "    test_feats = [(nbc_find_features(word, pos_tag), ne_tag) for (word, pos_tag, ne_tag) in testset]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_feats)\n",
    "    return train_feats, test_feats, classifier\n",
    "\n",
    "nbc_train_featuresets, nbc_test_featuresets, nbc_classifier = nbc_train_classifier(nbc_train, nbc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 96619 out of 114658 giving an accuracy of 84.27%\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC       0.47      0.82      0.60      4062\n",
      "       MISC       0.30      0.63      0.40      2680\n",
      "          O       0.99      0.87      0.92     96881\n",
      "        ORG       0.46      0.66      0.55      4689\n",
      "        PER       0.44      0.73      0.55      6346\n",
      "\n",
      "avg / total       0.90      0.84      0.86    114658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def nbc_evaluate(classifier, testset, test_featureset):   \n",
    "    num_correct = 0\n",
    "    pred_list = []\n",
    "    ground_truths = []\n",
    "    for i in range(len(testset)):\n",
    "        prediction = classifier.classify(nbc_find_features(testset[i][0], testset[i][1]))\n",
    "        pred_list.append(prediction)\n",
    "        ground_truths.append(testset[i][2])\n",
    "        if prediction == testset[i][2]:\n",
    "            num_correct = num_correct + 1\n",
    "    \n",
    "    acc = (nltk.classify.accuracy(classifier, test_featureset))*100\n",
    "    print('Predicted', num_correct, 'out of', len(testset), 'giving an accuracy of {}%'.format(round(acc, 2)))\n",
    "    print('\\n')\n",
    "    print(classification_report(ground_truths, pred_list))\n",
    "    \n",
    "nbc_evaluate(nbc_classifier, nbc_test, nbc_test_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kcnn_x, kcnn_y, kcnn_z = restructure(cleaned_conll_training + cleaned_neel_training) \n",
    "\n",
    "res = vectorize(kcnn_x)\n",
    "flattened = []\n",
    "for sent in res:\n",
    "    for wordvec in sent:\n",
    "        flattened.append(wordvec)\n",
    "flattened = np.array(flattened)\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=5)\n",
    "kmeans.fit(flattened)\n",
    "klabels = kmeans.labels_\n",
    "kcentroids = kmeans.cluster_centers_\n",
    "\n",
    "kcnn_x = pad_words(vectorize(kcnn_x))\n",
    "kcnn_train_x = kcnn_x.reshape(kcnn_x.shape[0], 3, 100, 1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(klabels)\n",
    "encoded_Y = encoder.transform(klabels)\n",
    "kcnn_train_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# Train Model & Save to File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# kcnn_model = train_cnn(kcnn_train_x, kcnn_train_y, 32, 10)\n",
    "# save_model(kcnn_model, 'kcnnmodel')\n",
    "\n",
    "# Load Model from File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "kcnn_model = load_model('kcnnmodel')\n",
    "kcnn_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Bootsrapping Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 28859 out of 114442 giving an accuracy of 25.22%\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC       0.08      0.35      0.13      4062\n",
      "       MISC       0.01      0.13      0.02      2680\n",
      "          O       0.84      0.24      0.37     96665\n",
      "        ORG       0.00      0.01      0.00      4689\n",
      "        PER       0.19      0.60      0.29      6346\n",
      "\n",
      "avg / total       0.73      0.25      0.34    114442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_evaluation(kcnn_model, cnn_test_x, cnn_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = int(len(kcnn_train_y) / 4)\n",
    "elabels = np.concatenate((np.concatenate((kcnn_train_y[:quarter], cnn_train_y[quarter:len(kcnn_train_y)-quarter])), kcnn_train_y[len(kcnn_train_y)-quarter:]))\n",
    "\n",
    "# Train Model & Save to File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# ecnn_model = train_cnn(kcnn_train_x, elabels, 32, 10)\n",
    "# save_model(ecnn_model, 'ecnnmodel')\n",
    "\n",
    "# Load Model from File\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "ecnn_model = load_model('ecnnmodel')\n",
    "ecnn_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 28500 out of 114442 giving an accuracy of 24.9%\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        LOC       0.07      0.44      0.11      4062\n",
      "       MISC       0.01      0.12      0.02      2680\n",
      "          O       1.00      0.24      0.38     96665\n",
      "        ORG       0.09      0.41      0.15      4689\n",
      "        PER       0.09      0.26      0.14      6346\n",
      "\n",
      "avg / total       0.85      0.25      0.34    114442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_evaluation(ecnn_model, cnn_test_x, cnn_test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
